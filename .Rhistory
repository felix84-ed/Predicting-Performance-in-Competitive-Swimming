)) %>%
fill(group, .direction = "downup") %>%
ungroup()
# Entfernen von Zeilen mit fehlenden Gruppenwerten
data_clean <- data %>% filter(!is.na(group))
# 4. Behandlung fehlender Werte: Ersetzen von NA-Werten in numerischen Variablen durch den Median
numeric_features <- c("Frühestes_Alter", "improvement_rate", "max_yearly_improvement",
"years_in_top100", "max_dist_categories", "max_disciplines",
"fina_16", "max_z_time_rate", "z_time_ratio", "jaehrliche_verbesserung_prozent")
data_clean[numeric_features] <- data_clean[numeric_features] %>%
mutate(across(everything(), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))
# Wandle die Zielvariable in einen Faktor um
data_clean$group <- as.factor(data_clean$group)
# 5. Stratifizierte Folds erstellen (sicherstellen, dass jede Gruppe in den Folds vertreten ist)
set.seed(123)
folds <- createFolds(data_clean$group, k = 5, list = TRUE, returnTrain = TRUE)
# 6. Klassengewichtung basierend auf der Anzahl der Beispiele pro Klasse
class_weights <- table(data_clean$group)
class_weights <- max(class_weights) / class_weights  # Umgekehrte Gewichtung
# 7. Folds durchlaufen und ROC-Kurven erstellen
roc_curves <- list()
auc_values <- c()
accuracy_values <- c()
precision_values <- list()
recall_values <- list()
f1_scores <- list()
for (i in 1:5) {
# Trainings- und Testdaten für den Fold erstellen
fold_train <- data_clean[folds[[i]], ]
fold_test <- data_clean[-folds[[i]], ]
# Random Forest mit Klassengewichtung trainieren
rf_model <- randomForest(group ~ Frühestes_Alter + improvement_rate + max_yearly_improvement +
years_in_top100 + max_dist_categories + max_disciplines +
fina_16 + max_z_time_rate + z_time_ratio,
data = fold_train, importance = TRUE, classwt = class_weights)
# Vorhersagen auf Wahrscheinlichkeitsbasis für den aktuellen Fold
rf_probabilities <- predict(rf_model, newdata = fold_test, type = "prob")
# ROC-Kurve und AUC für Gruppe 1 vs. Gruppe 2
roc_curve <- roc(fold_test$group, rf_probabilities[, 2], levels = c(1, 2), direction = "<")
auc_values[i] <- auc(roc_curve)
# Speichern der ROC-Kurve
roc_curves[[i]] <- roc_curve
# 10. Vorhersagen für die Testdaten (Klassen)
rf_class_predictions <- predict(rf_model, newdata = fold_test)  # Vorhersage der Klassen (1 oder 2)
# 11. Erstellen der Konfusionsmatrix
confusion_matrix_rf <- table(fold_test$group, rf_class_predictions)
print(paste("Konfusionsmatrix für Fold", i, " (Random Forest):"))
print(confusion_matrix_rf)
# 12. Berechnen der Accuracy
accuracy <- sum(diag(confusion_matrix_rf)) / sum(confusion_matrix_rf)
accuracy_values[i] <- accuracy
print(paste("Accuracy für Fold", i, ": ", round(accuracy, 4)))
# 13. Berechnen von Precision, Recall und F1-Score
precision <- diag(confusion_matrix_rf) / colSums(confusion_matrix_rf)
recall <- diag(confusion_matrix_rf) / rowSums(confusion_matrix_rf)
f1_score <- 2 * (precision * recall) / (precision + recall)
# Speichern von Precision, Recall und F1-Score für jeden Fold
precision_values[[i]] <- precision
recall_values[[i]] <- recall
f1_scores[[i]] <- f1_score
print(paste("Precision für Fold", i, ":"))
print(precision)
print(paste("Recall für Fold", i, ":"))
print(recall)
print(paste("F1-Score für Fold", i, ":"))
print(f1_score)
}
# 8. Durchschnitts-ROC und Standardabweichung berechnen
fpr_grid <- seq(0, 1, length.out = 100)  # Gemeinsame FPR-Achse
tpr_interpolated <- matrix(0, nrow = 5, ncol = length(fpr_grid))
for (i in 1:5) {
# Interpolieren der TPR-Werte auf dem gemeinsamen FPR-Gitter (verwende 1 - Specificities)
tpr_interpolated[i, ] <- approx(1 - roc_curves[[i]]$specificities, roc_curves[[i]]$sensitivities, xout = fpr_grid, ties = mean)$y
}
# 8. Durchschnittswerte für Accuracy, Precision, Recall und F1-Score berechnen
mean_accuracy <- mean(accuracy_values)
mean_precision <- sapply(precision_values, mean)
mean_recall <- sapply(recall_values, mean)
mean_f1_score <- sapply(f1_scores, mean)
print(paste("Durchschnittliche Accuracy: ", round(mean_accuracy, 4)))
print(paste("Durchschnittliche Precision: ", round(mean(mean_precision), 4)))
print(paste("Durchschnittlicher Recall: ", round(mean(mean_recall), 4)))
print(paste("Durchschnittlicher F1-Score: ", round(mean(mean_f1_score), 4)))
# 1. Laden der notwendigen Pakete
if(!require(dplyr)) {
install.packages("dplyr")
library(dplyr)
}
if(!require(tidyr)) {
install.packages("tidyr")
library(tidyr)
}
if(!require(glmnet)) {
install.packages("glmnet")
library(glmnet)
}
if(!require(pROC)) {
install.packages("pROC")
library(pROC)
}
if(!require(caret)) {
install.packages("caret")
library(caret)
}
# 2. Laden des Datensatzes
data <- read.csv("/Users/fbongarz/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/Data/extended_dataset_processed.csv")
# 3. Definieren der Zielvariable basierend auf den FINA-Punkten im Alter von 18 Jahren (nur zwei Gruppen)
data <- data %>%
group_by(unique_id, Event) %>%
mutate(group = case_when(
Fina < 750 & Alter == 18 ~ 2,    # Gruppe 2: FINA < 750
Fina >= 750 & Alter == 18 ~ 1,   # Gruppe 1: FINA >= 750
TRUE ~ NA_real_  # Setzt NA für alle anderen Altersgruppen
)) %>%
fill(group, .direction = "downup") %>%
ungroup()
# Entfernen von Zeilen mit fehlenden Gruppenwerten
data_clean <- data %>% filter(!is.na(group))
# 4. Behandlung fehlender Werte: Ersetzen von NA-Werten in numerischen Variablen durch den Median
numeric_features <- c("Frühestes_Alter", "improvement_rate", "max_yearly_improvement",
"years_in_top100", "max_dist_categories", "max_disciplines",
"fina_16", "max_z_time_rate", "z_time_ratio", "jaehrliche_verbesserung_prozent")
data_clean[numeric_features] <- data_clean[numeric_features] %>%
mutate(across(everything(), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))
# Wandle die Zielvariable in einen Faktor um
data_clean$group <- as.factor(data_clean$group)
# 5. Stratifizierte Folds erstellen (sicherstellen, dass jede Gruppe in den Folds vertreten ist)
set.seed(123)
folds <- createFolds(data_clean$group, k = 5, list = TRUE, returnTrain = TRUE)
# 6. Klassengewichtung basierend auf der Anzahl der Beispiele pro Klasse
class_weights <- table(data_clean$group)
class_weights <- max(class_weights) / class_weights  # Umgekehrte Gewichtung
# 7. Folds durchlaufen und ROC-Kurven erstellen
roc_curves <- list()
auc_values <- c()
accuracy_values <- c()
precision_values <- list()
recall_values <- list()
f1_scores <- list()
for (i in 1:5) {
# Trainings- und Testdaten für den Fold erstellen
fold_train <- data_clean[folds[[i]], ]
fold_test <- data_clean[-folds[[i]], ]
# Um die Daten mit glmnet zu verwenden, müssen sie in Matrix-Form vorliegen
x_train <- as.matrix(fold_train[, numeric_features])
y_train <- as.numeric(fold_train$group) - 1  # glmnet benötigt binäre Zielvariablen (0, 1)
# Regularisierte Logistische Regression mit Ridge-Regularisierung (alpha = 0 für Ridge)
# Finde das optimale Lambda mit Kreuzvalidierung
cv_fit <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 0, weights = ifelse(fold_train$group == 1, class_weights[1], class_weights[2]))
# Vorhersagen auf Wahrscheinlichkeitsbasis für den aktuellen Fold (Verwendung von lambda.min)
lr_probabilities <- predict(cv_fit, newx = as.matrix(fold_test[, numeric_features]), type = "response", s = "lambda.min")
# Extrahiere den Vektor aus der Matrix
lr_probabilities <- as.vector(lr_probabilities)
# ROC-Kurve und AUC für Gruppe 1 vs. Gruppe 2
roc_curve <- roc(fold_test$group, lr_probabilities, levels = c(1, 2), direction = "<")  # Richtung für die ROC-Kurve angepasst
auc_values[i] <- auc(roc_curve)
# Speichern der ROC-Kurve
roc_curves[[i]] <- roc_curve
# 10. Vorhersagen für die Testdaten (Klassen)
lr_class_predictions <- predict(cv_fit, newdata = fold_test)  # Vorhersage der Klassen (1 oder 2)
# 11. Erstellen der Konfusionsmatrix
confusion_matrix_lr <- table(fold_test$group, lr_class_predictions)
print(paste("Konfusionsmatrix für Fold", i, " (Log):"))
print(confusion_matrix_lr)
# 12. Berechnen der Accuracy
accuracy <- sum(diag(confusion_matrix_lr)) / sum(confusion_matrix_lr)
accuracy_values[i] <- accuracy
print(paste("Accuracy für Fold", i, ": ", round(accuracy, 4)))
}
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/ML_LR_Gewichtung_ROC.R", echo=TRUE)
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/ML_Rf_Gewichtung_ROC.R", echo=TRUE)
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/ML_KNN_Gewichtung_ROC.R", echo=TRUE)
# 7. Folds parallel durchlaufen und Metriken berechnen
auc_values <- foreach(i = 1:5, .combine = c, .packages = c('class', 'pROC', 'dplyr')) %dopar% {
# Trainings- und Testdaten für den Fold erstellen
fold_train <- data_clean[folds[[i]], ]
fold_test <- data_clean[-folds[[i]], ]
# Erstellen von Trainings- und Testdatensätzen
x_train <- as.matrix(fold_train[, numeric_features])
x_test <- as.matrix(fold_test[, numeric_features])
y_train <- as.factor(fold_train$group)
y_test <- as.factor(fold_test$group)
# KNN-Modell mit k = 7
k <- 7
knn_predictions <- knn(train = x_train, test = x_test, cl = y_train, k = k, prob = TRUE)
# Wahrscheinlichkeiten für KNN ermitteln
knn_probabilities <- attr(knn_predictions, "prob")
if (is.null(knn_probabilities)) {
print(paste("Fold", i, ": Keine Wahrscheinlichkeiten berechnet"))
return(NA)  # Skip if no probabilities
}
# Wahrscheinlichkeiten zu Gruppe 2 (positiv) zuordnen
knn_probabilities <- ifelse(knn_predictions == 2, knn_probabilities, 1 - knn_probabilities)
# ROC-Kurve und AUC berechnen
roc_curve <- roc(y_test, knn_probabilities, levels = c(1, 2), direction = "<")
auc_value <- auc(roc_curve)
# Vorhersagen als Klassen (1 oder 2)
knn_class_predictions <- knn(train = x_train, test = x_test, cl = y_train, k = k)
# Konfusionsmatrix
confusion_matrix_knn <- table(y_test, knn_class_predictions)
# Überprüfen, ob Konfusionsmatrix korrekt ist (nicht leer)
if (sum(confusion_matrix_knn) == 0) {
print(paste("Fold", i, ": Keine gültige Konfusionsmatrix"))
return(NA)
}
# Accuracy
accuracy <- sum(diag(confusion_matrix_knn)) / sum(confusion_matrix_knn)
accuracy_values[i] <- accuracy
# Precision, Recall, F1-Score
precision <- diag(confusion_matrix_knn) / colSums(confusion_matrix_knn)
recall <- diag(confusion_matrix_knn) / rowSums(confusion_matrix_knn)
f1_score <- 2 * (precision * recall) / (precision + recall)
# Überprüfen, ob Werte NA sind (falls dies der Fall ist, überspringen)
if (any(is.na(c(accuracy, precision, recall, f1_score)))) {
print(paste("Fold", i, ": Ungültige Metriken berechnet"))
return(NA)
}
# Speichern der Metriken
precision_values[[i]] <- precision
recall_values[[i]] <- recall
f1_scores[[i]] <- f1_score
return(auc_value)
}
# 7. Folds durchlaufen und Metriken berechnen (ohne paralleles Rechnen)
for (i in 1:5) {
# Trainings- und Testdaten für den Fold erstellen
fold_train <- data_clean[folds[[i]], ]
fold_test <- data_clean[-folds[[i]], ]
# Erstellen von Trainings- und Testdatensätzen
x_train <- as.matrix(fold_train[, numeric_features])
x_test <- as.matrix(fold_test[, numeric_features])
y_train <- as.factor(fold_train$group)
y_test <- as.factor(fold_test$group)
# KNN-Modell mit k = 7
k <- 7
knn_predictions <- knn(train = x_train, test = x_test, cl = y_train, k = k, prob = TRUE)
# Wahrscheinlichkeiten für KNN ermitteln
knn_probabilities <- attr(knn_predictions, "prob")
if (is.null(knn_probabilities)) {
print(paste("Fold", i, ": Keine Wahrscheinlichkeiten berechnet"))
next  # Falls keine Wahrscheinlichkeiten berechnet werden, überspringe den Fold
}
# Wahrscheinlichkeiten zu Gruppe 2 (positiv) zuordnen
knn_probabilities <- ifelse(knn_predictions == 2, knn_probabilities, 1 - knn_probabilities)
# ROC-Kurve und AUC berechnen
roc_curve <- roc(y_test, knn_probabilities, levels = c(1, 2), direction = "<")
auc_value <- auc(roc_curve)
# Vorhersagen als Klassen (1 oder 2)
knn_class_predictions <- knn(train = x_train, test = x_test, cl = y_train, k = k)
# Konfusionsmatrix
confusion_matrix_knn <- table(y_test, knn_class_predictions)
# Überprüfen, ob Konfusionsmatrix korrekt ist (nicht leer)
if (sum(confusion_matrix_knn) == 0) {
print(paste("Fold", i, ": Keine gültige Konfusionsmatrix"))
next
}
# Accuracy
accuracy <- sum(diag(confusion_matrix_knn)) / sum(confusion_matrix_knn)
accuracy_values[i] <- accuracy
# Precision, Recall, F1-Score
precision <- diag(confusion_matrix_knn) / colSums(confusion_matrix_knn)
recall <- diag(confusion_matrix_knn) / rowSums(confusion_matrix_knn)
f1_score <- 2 * (precision * recall) / (precision + recall)
# Überprüfen, ob Werte NA sind (falls dies der Fall ist, überspringen)
if (any(is.na(c(accuracy, precision, recall, f1_score)))) {
print(paste("Fold", i, ": Ungültige Metriken berechnet"))
next
}
# Speichern der Metriken
precision_values[[i]] <- precision
recall_values[[i]] <- recall
f1_scores[[i]] <- f1_score
auc_values[i] <- auc_value
}
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/ML_KNN_Gewichtung_ROC.R", echo=TRUE)
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/ML_KNN_Gewichtung_ROC.R", echo=TRUE)
# 6. Folds durchlaufen und Metriken berechnen
accuracy_values <- c()
precision_values <- list()
recall_values <- list()
f1_scores <- list()
roc_curves <- list()  # Für die ROC-Kurven
auc_values <- c()
confusion_matrices <- list()  # Liste für Konfusionsmatrizen
for (i in 1:5) {
# Trainings- und Testdaten für den Fold erstellen
fold_train <- data_clean[folds[[i]], ]
fold_test <- data_clean[-folds[[i]], ]
# Erstellen von Trainings- und Testdatensätzen
x_train <- as.matrix(fold_train[, numeric_features])
x_test <- as.matrix(fold_test[, numeric_features])
y_train <- as.factor(fold_train$group)
y_test <- as.factor(fold_test$group)
# Kleine Störung zu den Trainings- und Testdaten hinzufügen, um Ties zu vermeiden
x_train <- x_train + matrix(runif(n = length(x_train), min = -1e-6, max = 1e-6), nrow = nrow(x_train))
x_test <- x_test + matrix(runif(n = length(x_test), min = -1e-6, max = 1e-6), nrow = nrow(x_test))
# Normalisiere die Daten
x_train <- scale(x_train)
x_test <- scale(x_test)
# KNN-Modell mit k = 10
k <- 10
knn_predictions <- knn(train = x_train, test = x_test, cl = y_train, k = k, prob = TRUE)
# Wahrscheinlichkeiten für KNN ermitteln
knn_probabilities <- attr(knn_predictions, "prob")
if (is.null(knn_probabilities)) {
print(paste("Fold", i, ": Keine Wahrscheinlichkeiten berechnet"))
next  # Falls keine Wahrscheinlichkeiten berechnet werden, überspringe den Fold
}
# Wahrscheinlichkeiten zu Gruppe 2 (positiv) zuordnen
knn_probabilities <- ifelse(knn_predictions == 2, knn_probabilities, 1 - knn_probabilities)
# ROC-Kurve und AUC berechnen
roc_curve <- roc(y_test, knn_probabilities, levels = c(1, 2), direction = "<")
auc_value <- auc(roc_curve)
# Speichern der ROC-Kurve und AUC-Werte
roc_curves[[i]] <- roc_curve
auc_values[i] <- auc_value
# Vorhersagen als Klassen (1 oder 2)
knn_class_predictions <- knn(train = x_train, test = x_test, cl = y_train, k = k)
# Konfusionsmatrix
confusion_matrix_knn <- table(y_test, knn_class_predictions)
confusion_matrices[[i]] <- confusion_matrix_knn  # Speichern der Konfusionsmatrix für jeden Fold
# Accuracy
accuracy <- sum(diag(confusion_matrix_knn)) / sum(confusion_matrix_knn)
accuracy_values[i] <- accuracy
# Precision, Recall, F1-Score
precision <- diag(confusion_matrix_knn) / colSums(confusion_matrix_knn)
recall <- diag(confusion_matrix_knn) / rowSums(confusion_matrix_knn)
f1_score <- 2 * (precision * recall) / (precision + recall)
# Speichern der Metriken
precision_values[[i]] <- precision
recall_values[[i]] <- recall
f1_scores[[i]] <- f1_score
}
# 7. Durchschnittswerte berechnen
mean_accuracy <- ifelse(length(accuracy_values) > 0, mean(accuracy_values, na.rm = TRUE), NA)
mean_precision <- ifelse(length(precision_values) > 0, sapply(precision_values, mean, na.rm = TRUE), NA)
mean_recall <- ifelse(length(recall_values) > 0, sapply(recall_values, mean, na.rm = TRUE), NA)
mean_f1_score <- ifelse(length(f1_scores) > 0, sapply(f1_scores, mean, na.rm = TRUE), NA)
# Ausgabe der Ergebnisse
print(paste("Durchschnittliche Accuracy: ", round(mean_accuracy, 4)))
print(paste("Durchschnittliche Precision: ", round(mean(mean_precision), 4)))
print(paste("Durchschnittlicher Recall: ", round(mean(mean_recall), 4)))
print(paste("Durchschnittlicher F1-Score: ", round(mean(mean_f1_score), 4)))
# 8. Konfusionsmatrix anzeigen
print("Konfusionsmatrix für jeden Fold:")
for (i in 1:5) {
print(paste("Konfusionsmatrix für Fold", i, ":"))
print(confusion_matrices[[i]])
}
# Speichern der Konfusionsmatrix in einer Datei (optional)
confusion_matrix_df <- do.call(rbind, lapply(1:5, function(i) as.data.frame(confusion_matrices[[i]])))
write.csv(confusion_matrix_df, "/Users/fbongarz/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/Data/Ergebnisse/knn_confusion_matrix.csv", row.names = FALSE)
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/ML_KNN_Gewichtung_ROC.R", echo=TRUE)
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/ML_KNN_Gewichtung_ROC.R", echo=TRUE)
# 3. Definieren der Zielvariable basierend auf den FINA-Punkten im Alter von 18 Jahren (nur zwei Gruppen)
data <- data %>%
group_by(unique_id, Event) %>%
mutate(group = case_when(
Fina >= 750 & Alter == 18 ~ 1,   # Gruppe 1: FINA >= 750
Fina < 750 & Alter == 18 ~ 0,    # Gruppe 2: FINA < 750
TRUE ~ NA_real_  # Setzt NA für alle anderen Altersgruppen
)) %>%
fill(group, .direction = "downup") %>%
ungroup()
# 4. Entfernen von Zeilen mit fehlenden Gruppenwerten
data_clean <- data %>% filter(!is.na(group))
# 5. Behandlung fehlender Werte
# ... (wie oben)
# 6. Stratifizierte Folds erstellen (mit korrekten Levels)
set.seed(123)
folds <- createFolds(data_clean$group, k = 5, list = TRUE, returnTrain = TRUE)
# 7. Folds durchlaufen und Metriken berechnen
accuracy_values <- c()
precision_values <- list()
recall_values <- list()
f1_scores <- list()
roc_curves <- list()  # Für die ROC-Kurven
auc_values <- c()
confusion_matrices <- list()  # Liste für Konfusionsmatrizen
for (i in 1:5) {
# Trainings- und Testdaten für den Fold erstellen
fold_train <- data_clean[folds[[i]], ]
fold_test <- data_clean[-folds[[i]], ]
# Erstellen von Trainings- und Testdatensätzen
x_train <- as.matrix(fold_train[, numeric_features])
x_test <- as.matrix(fold_test[, numeric_features])
y_train <- as.factor(fold_train$group)
y_test <- as.factor(fold_test$group)
# Kleine Störung zu den Trainings- und Testdaten hinzufügen, um Ties zu vermeiden
x_train <- x_train + matrix(runif(n = length(x_train), min = -1e-6, max = 1e-6), nrow = nrow(x_train))
x_test <- x_test + matrix(runif(n = length(x_test), min = -1e-6, max = 1e-6), nrow = nrow(x_test))
# Normalisiere die Daten
x_train <- scale(x_train)
x_test <- scale(x_test)
# KNN-Modell mit k = 10
k <- 10
knn_predictions <- knn(train = x_train, test = x_test, cl = y_train, k = k, prob = TRUE)
# Wahrscheinlichkeiten für KNN ermitteln
knn_probabilities <- attr(knn_predictions, "prob")
if (is.null(knn_probabilities)) {
print(paste("Fold", i, ": Keine Wahrscheinlichkeiten berechnet"))
next  # Falls keine Wahrscheinlichkeiten berechnet werden, überspringe den Fold
}
# Wahrscheinlichkeiten zu Gruppe 1 (positiv) zuordnen
knn_probabilities <- ifelse(knn_predictions == 1, knn_probabilities, 1 - knn_probabilities)
# ROC-Kurve und AUC berechnen
roc_curve <- roc(y_test, knn_probabilities, levels = c(0, 1), direction = "<")  # ACHTUNG: levels angepasst
auc_value <- auc(roc_curve)
# Speichern der ROC-Kurve und AUC-Werte
roc_curves[[i]] <- roc_curve
auc_values[i] <- auc_value
# Vorhersagen als Klassen (0 oder 1)
knn_class_predictions <- knn(train = x_train, test = x_test, cl = y_train, k = k)
# Konfusionsmatrix
confusion_matrix_knn <- table(y_test, knn_class_predictions)
confusion_matrices[[i]] <- confusion_matrix_knn  # Speichern der Konfusionsmatrix für jeden Fold
# Accuracy
accuracy <- sum(diag(confusion_matrix_knn)) / sum(confusion_matrix_knn)
accuracy_values[i] <- accuracy
# Precision, Recall, F1-Score
precision <- diag(confusion_matrix_knn) / colSums(confusion_matrix_knn)
recall <- diag(confusion_matrix_knn) / rowSums(confusion_matrix_knn)
f1_score <- 2 * (precision * recall) / (precision + recall)
# Speichern der Metriken
precision_values[[i]] <- precision
recall_values[[i]] <- recall
f1_scores[[i]] <- f1_score
}
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/ML_KNN_Gewichtung_ROC.R", echo=TRUE)
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/ML_KNN_Gewichtung_ROC.R", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/ML_NN_Gewichtung_ROC.R", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/ML-Algo_Rf_Gewichtung.R", echo=TRUE)
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/ML-Algo_Rf.R", echo=TRUE)
# 13. Optional: Feature Importance analysieren und anzeigen
importance(rf_model)
# Anpassung der Ränder des Plots (unten, links, oben, rechts)
par(mar = c(5, 5, 4, 2))  # Passe die Werte an, um die Grafik zu verkleinern
# Erneutes Erstellen des varImpPlots
varImpPlot(rf_model)
# Grafik in eine Datei speichern
png("/Users/fbongarz/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/Data/feature_importance.png", width = 800, height = 600)
varImpPlot(rf_model)
dev.off()
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/LassoRegression.R", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/ML_Rf_Gewichtung_ROC.R", echo=TRUE)
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/ML_KNN_Gewichtung_ROC.R", echo=TRUE)
# 13. Optional: Feature Importance analysieren und anzeigen
importance(knn_probabilities)
# 10. Speichern der Tabelle mit AUC-Werten als CSV
results <- data.frame(Fold = 1:5, AUC = auc_values)
results$Mean_AUC <- mean(auc_values)
results$Std_AUC <- sd(auc_values)
results$Mean_Accuracy <- mean_accuracy
results$Mean_Precision <- mean(mean_precision)
results$Mean_Recall <- mean(mean_recall)
results$Mean_F1_Score <- mean(mean_f1_score)
write.csv(results, "/Users/fbongarz/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/Data/Ergebnisse/knn_neu_results.csv", row.names = FALSE)
results$Sd_Accuracy <- sd(mean_accuracy)
# 10. Speichern der Tabelle mit AUC-Werten als CSV
results <- data.frame(Fold = 1:5, AUC = auc_values)
results$Mean_AUC <- mean(auc_values)
results$Std_AUC <- sd(auc_values)
results$Mean_Accuracy <- mean_accuracy
results$Sd_Accuracy <- sd(mean_accuracy)
results$Mean_Precision <- mean(mean_precision)
results$Mean_Recall <- mean(mean_recall)
results$Mean_F1_Score <- mean(mean_f1_score)
write.csv(results, "/Users/fbongarz/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/Data/Ergebnisse/knn_neu_results.csv", row.names = FALSE)
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/LassoRegression.R", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
# Berechnung der Ergebnisse für Variable, B, SD, Z-Wert, p-Wert, 95% Konfidenzintervall, e^B, und 95% Konfidenzintervall für e^B
coefficients_summary <- summary(logit_model)
# Extrahiere Koeffizienten, Standardfehler, Z-Wert und P-Wert
coefficients_table <- as.data.frame(coefficients_summary$coefficients)
coefficients_table$Variable <- rownames(coefficients_table)
# Berechne das Konfidenzintervall für die Koeffizienten
confint_values <- confint(logit_model)
# Berechne Odds Ratio (e^B) und das Konfidenzintervall dafür
odds_ratio <- exp(coefficients_table[,1])
odds_ratio_confint <- exp(confint_values)
# Füge alle Daten in einer Tabelle zusammen
final_table <- data.frame(
Variable = rownames(coefficients_table),
B = coefficients_table[,1],            # Koeffizient
SD = coefficients_table[,2],           # Standardfehler
Z_Value = coefficients_table[,3],      # Z-Wert
p_value = coefficients_table[,4],      # p-Wert
`95% CI (B)` = paste0(round(confint_values[,1], 4), " to ", round(confint_values[,2], 4)),  # 95% Konfidenzintervall für B
`e^B` = round(odds_ratio, 4),          # e^B (Odds Ratio)
`95% CI (e^B)` = paste0(round(odds_ratio_confint[,1], 4), " to ", round(odds_ratio_confint[,2], 4))  # 95% Konfidenzintervall für e^B
)
# Zeige die finale Tabelle an
print(final_table)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/Library/Mobile Documents/com~apple~CloudDocs/02_Studium/Bachelor-Arbeit/BachelorArbeit/Schritt4.R", echo=TRUE)
